# Awesome RL-VLA ü§ñ

A curated list of papers and resources on **Reinforcement Learning for Vision-Language-Action (RL-VLA)** models. This repository provides a comprehensive overview of training paradigms, methodologies, and state-of-the-art approaches in RL-VLA research.

## üìñ Table of Contents
- [Overview](#overview)
- [Training Paradigms](#training-paradigms)
  - [Offline RL-VLA](#offline-rl-vla)
  - [Online RL-VLA](#online-rl-vla)
  - [Test-time RL-VLA](#test-time-rl-vla)
- [Paper Collection](#paper-collection)
- [Contributing](#contributing)

## üîç Overview

RL training is crucial for enabling VLAs to generalize out-of-distribution (OOD) from large-scale pre-trained data. Existing RL-VLA training paradigms can be categorized into three types based on how agents obtain and utilize feedback from the environment:

- **Online RL-VLA**: Direct interaction with the environment during training
- **Offline RL-VLA**: Learning from static datasets without further environmental interaction  
- **Test-time RL-VLA**: Models adapt their behavior during deployment without altering parameters

## üöÄ Training Paradigms

### Offline RL-VLA

Offline RL trains VLA models on pre-collected static datasets, enabling learning independently from environment interactions. This paradigm is suitable for high-risk or resource-constrained deployment scenarios.

**Key Research Directions:**
- **Data Utilization**: Effective utilization of static datasets for policy improvement
- **Objective Modification**: Customizing RL objectives for novel architectures and data augmentation

### Online RL-VLA

Online RL-VLA enables interactive policy learning through continuous environment interaction, empowering pre-trained VLAs with adaptive closed-loop control capability for real-world OOD environments.

**Key Research Directions:**
- **Policy Optimization**: Direct policy improvement based on environmental rewards
- **Sample Efficiency**: Learning effective policies with limited interaction budget
- **Active Exploration**: Efficient exploration strategies for higher performance gains
- **Training Stability**: Ensuring consistent policy updates and convergence
- **Infrastructure**: Scalable frameworks for online RL-VLA training

### Test-time RL-VLA

Test-time RL-VLA adapts behavior during deployment through lightweight updates, addressing the expensive cost of full model fine-tuning in real-world scenarios.

**Key Adaptation Mechanisms:**
- **Value Guidance**: Using pre-trained value functions to influence action selection
- **Memory Buffer Guidance**: Retrieving relevant historical experiences during inference
- **Planning-guided Adaptation**: Explicit reasoning over future action sequences

## üìö Paper Collection

### Legend
- **Action**: AR (Autoregressive), Diffusion, Flow (Flow-matching)
- **Reward**: D (Dense Reward), S (Sparse Reward)
- **Type**: MB (Model-based), MF (Model-free)
- **Environment**: Sim. (Simulation), Real (Real-world)

### Offline RL-VLA

| Method | Date | Sim. | Real | Base VLA Model | Action | Reward | Algorithm | Type |
|--------|------|------|------|----------------|--------|---------|-----------|------|
| [Q-Transformer](https://arxiv.org/abs/2309.10150) | 2023.10 | ‚úì | ‚úó | Transformer | AR | S | CQL | MF |
| [PAC](https://arxiv.org/abs/2402.05546) | 2024.02 | ‚úì | ‚úì | Perceiver-Actor-Critic | AR | S | AC | MF |
| [ConRFT](https://arxiv.org/pdf/2502.05450) | 2025.04 | ‚úó | ‚úì | Octo-small | Diffusion | S | Cal-QL + BC | MF |
| [ReinboT](https://icml.cc/virtual/2025/poster/45523) | 2025.05 | ‚úì | ‚úì | ReinboT | AR | D | DT + RTG | MF |
| [CO-RFT](https://arxiv.org/pdf/2508.02219) | 2025.08 | ‚úó | ‚úì | RoboVLMs | AR | D | Cal-QL + TD3 | MF |
| [ARFM](https://arxiv.org/pdf/2509.04063) | 2025.09 | ‚úì | ‚úì | œÄ‚ÇÄ | Flow | D | ARFM | MF |

### Online RL-VLA

| Method | Date | Sim. | Real | Base VLA Model | Action | Reward | Algorithm | Type |
|--------|------|------|------|----------------|--------|---------|-----------|------|
| [FLaRe](https://arxiv.org/abs/2409.16578) | 2024.09 | ‚úì | ‚úì | SPOC | AR | S | PPO | MF |
| [PA-RL](https://arxiv.org/abs/2412.06685) | 2024.12 | ‚úì | ‚úì | OpenVLA | AR | S | PA-RL | MF |
| [RLDG](https://arxiv.org/pdf/2412.09858) | 2024.12 | ‚úó | ‚úì | OpenVLA / Octo | AR / Diffusion | S | RLPD | MF |
| [iRe-VLA](https://arxiv.org/abs/2501.16664) | 2025.01 | ‚úì | ‚úì | iRe-VLA | AR | S | SACfD + SFT | MF |
| [GRAPE](https://arxiv.org/pdf/2411.19309) | 2025.02 | ‚úì | ‚úì | OpenVLA | AR | D | TPO | MF |
| [SafeVLA](https://arxiv.org/abs/2503.03480) | 2025.03 | ‚úì | ‚úó | SPOC | AR | S | PPO | MF |
| [ConRFT](https://arxiv.org/pdf/2502.05450) | 2025.04 | ‚úó | ‚úì | Octo-small | Diffusion | S | Cal-QL + BC | MF |
| [RIPT-VLA](https://arxiv.org/abs/2505.17016) | 2025.05 | ‚úì | ‚úó | QueST / OpenVLA-OFT | AR | S | LOOP | MF |
| [VLA-RL](https://arxiv.org/abs/2505.18719) | 2025.05 | ‚úì | ‚úó | OpenVLA | AR | D | PPO | MF |
| [RLVLA](https://arxiv.org/abs/2505.19789) | 2025.05 | ‚úì | ‚úó | OpenVLA | AR | S | PPO / GRPO / DPO | MF |
| [RFTF](https://arxiv.org/abs/2505.19767) | 2025.05 | ‚úì | ‚úó | GR-MG, Seer | AR | D | PPO | MF |
| [TGRPO](https://arxiv.org/abs/2506.08440) | 2025.06 | ‚úì | ‚úó | OpenVLA | AR | D | GRPO | MF |
| [RLRC](https://arxiv.org/pdf/2506.17639) | 2025.06 | ‚úì | ‚úó | OpenVLA | AR | S | PPO | MF |
| [SimpleVLA-RL](https://arxiv.org/pdf/2509.09674) | 2025.09 | ‚úì | ‚úì | OpenVLA-OFT | AR | S | GRPO | MF |
| [Dual-Actor FT](https://arxiv.org/pdf/2509.13774) | 2025.09 | ‚úì | ‚úì | Octo / SmolVLA | Diffusion | S | QL + BC | MF |
| [Generalist](https://arxiv.org/pdf/2509.15155) | 2025.09 | ‚úì | ‚úì | PaLI 3B | AR | D | REINFORCE | MF |
| [VLAC](https://arxiv.org/abs/2509.15937) | 2025.09 | ‚úó | ‚úì | VLAC | AR | D | PPO | MF |
| [AC PPO](https://arxiv.org/pdf/2509.25718) | 2025.09 | ‚úì | ‚úó | Octo-small | AR | S | PPO+BC | MF |
| [VLA-RFT](https://arxiv.org/abs/2510.00406) | 2025.10 | ‚úì | ‚úó | VLA-Adapter | Flow | D | GRPO | MB |
| [RLinf-VLA](https://arxiv.org/pdf/2510.06710v1) | 2025.10 | ‚úì | ‚úì | OpenVLA / OpenVLA-OFT | AR | S | PPO / GRPO | MF |
| [FPO](https://arxiv.org/pdf/2510.09976) | 2025.10 | ‚úì | ‚úó | œÄ‚ÇÄ | Flow | S | FPO | MF |
| [ReSA](https://arxiv.org/pdf/2510.12710) | 2025.10 | ‚úì | ‚úó | OpenVLA | AR | D | PPO + SFT | MF |
| [œÄ_RL](https://arxiv.org/abs/2510.25889) | 2025.10 | ‚úì | ‚úó | œÄ‚ÇÄ / œÄ‚ÇÄ.‚ÇÖ | Flow | S | PPO / GRPO | MF |
| [PLD](https://arxiv.org/abs/2511.00091) | 2025.10 | ‚úì | ‚úì | OpenVLA / œÄ‚ÇÄ / Octo | AR / Flow | S | Cal-QL + SAC | MF |
| [World-Env](https://arxiv.org/abs/2509.24948) | 2025.11 | ‚úì | ‚úì | OpenVLA-OFT | AR | D | PPO | MB |
| [RobustVLA](https://arxiv.org/pdf/2511.01331) | 2025.11 | ‚úì | ‚úó | OpenVLA-OFT | AR | D | PPO | MF |
| [WMPO](https://arxiv.org/abs/2511.09515) | 2025.11 | ‚úì | ‚úì | OpenVLA-OFT | AR | S | GRPO | MB |

### Test-time RL-VLA

| Method | Date | Sim. | Real | Base VLA Model | Action | Reward | Algorithm | Type |
|--------|------|------|------|----------------|--------|---------|-----------|------|
| [V-GPS](https://arxiv.org/abs/2410.13816) | 2024.10 | ‚úì | ‚úì | Octo / RT-1 / OpenVLA | AR | D | Cal-QL | MF |
| [Hume](https://arxiv.org/abs/2505.21432) | 2025.06 | ‚úì | ‚úì | Hume | Flow | S | Value Guidance | MF |
| [VLA-Reasoner](https://arxiv.org/abs/2509.22643) | 2025.09 | ‚úì | ‚úì | OpenVLA / SpatialVLA et al. | AR / Diffusion | D | MCTS | MB |

## üîó Useful Resources

### Base VLA Models
- [OpenVLA](https://github.com/openvla/openvla) - Open-source VLA model
- [Octo](https://github.com/octo-models/octo) - Generalist robot policy
- [RT-1](https://github.com/google-research/robotics_transformer) - Robotics Transformer

### Datasets & Benchmarks
- [Open X-Embodiment](https://robotics-transformer-x.github.io/) - Large-scale robotic datasets
- [LIBERO](https://libero-ai.github.io/) - Benchmark for lifelong robot learning

### Frameworks & Tools
- [RLinf](https://github.com/RLinf/RLinf) - Infrastructure for online RL fine-tuning of VLAs

## ü§ù Contributing

We welcome contributions to this awesome list! Please feel free to:

1. **Add new papers**: Submit a PR with new RL-VLA papers following the existing format
2. **Update information**: Correct any errors or update paper information
3. **Suggest improvements**: Propose better organization or additional sections

### Contribution Guidelines
- Ensure papers are relevant to RL-VLA research
- Include paper links, project pages (if available), and key details
- Follow the existing table format for consistency
- Add a brief description for new paradigms or significant methodological contributions

## üìÑ Citation

If you find this repository useful, please consider citing:

```bibtex
@article{deng2025,
  title={MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation},
  author={Li, Runhao and Goo, Wenkai and Wu, Zhenyu and Wang, Changyuan and Deng, Haoyuan and Weng, Zhenyu and Tan, Yap-Peng and Wang, Ziwei},
  journal={arXiv preprint arXiv:2511.09516},
  year={2025}
}
```

## üìä Statistics

- **Total Papers**: 40+
- **Offline RL-VLA**: 6 papers
- **Online RL-VLA**: 30+ papers  
- **Test-time RL-VLA**: 3 papers
- **Last Updated**: November 2025

---

‚≠ê **Star this repository** if you find it helpful!

üîÑ **Watch** for updates on the latest RL-VLA research!