# Awesome RL-VLA ğŸ¤–

A curated list of papers and resources on **Reinforcement Learning for Vision-Language-Action (RL-VLA)** models. This repository provides a comprehensive overview of training paradigms, methodologies, and state-of-the-art approaches in RL-VLA research.

## ğŸ“– Table of Contents
- [Awesome RL-VLA ğŸ¤–](#awesome-rl-vla-)
  - [ğŸ“– Table of Contents](#-table-of-contents)
  - [ğŸ” Overview](#-overview)
  - [ğŸš€ Training Paradigms](#-training-paradigms)
    - [Offline RL-VLA](#offline-rl-vla)
    - [Online RL-VLA](#online-rl-vla)
    - [Test-time RL-VLA](#test-time-rl-vla)
  - [ğŸ“š Paper Collection](#-paper-collection)
    - [Legend](#legend)
    - [Offline RL-VLA](#offline-rl-vla-1)
    - [Online RL-VLA](#online-rl-vla-1)
    - [Test-time RL-VLA](#test-time-rl-vla-1)
  - [ğŸ”— Useful Resources](#-useful-resources)
    - [Base VLA Models](#base-vla-models)
    - [Datasets \& Benchmarks](#datasets--benchmarks)
    - [Frameworks \& Tools](#frameworks--tools)
  - [ğŸ¤ Contributing](#-contributing)
    - [Contribution Guidelines](#contribution-guidelines)
  - [ğŸ“„ Citation](#-citation)

## ğŸ” Overview

RL training is crucial for enabling VLAs to generalize out-of-distribution (OOD) from large-scale pre-trained data. Existing RL-VLA training paradigms can be categorized into three types based on how agents obtain and utilize feedback from the environment:

- **Online RL-VLA**: Direct interaction with the environment during training
- **Offline RL-VLA**: Learning from static datasets without further environmental interaction  
- **Test-time RL-VLA**: Models adapt their behavior during deployment without altering parameters

## ğŸš€ Training Paradigms

### Offline RL-VLA

Offline RL trains VLA models on pre-collected static datasets, enabling learning independently from environment interactions. This paradigm is suitable for high-risk or resource-constrained deployment scenarios.

**Key Research Directions:**
- **Data Utilization**: Effective utilization of static datasets for policy improvement
- **Objective Modification**: Customizing RL objectives for novel architectures and data augmentation

### Online RL-VLA

Online RL-VLA enables interactive policy learning through continuous environment interaction, empowering pre-trained VLAs with adaptive closed-loop control capability for real-world OOD environments.

**Key Research Directions:**
- **Policy Optimization**: Direct policy improvement based on environmental rewards
- **Sample Efficiency**: Learning effective policies with limited interaction budget
- **Active Exploration**: Efficient exploration strategies for higher performance gains
- **Training Stability**: Ensuring consistent policy updates and convergence
- **Infrastructure**: Scalable frameworks for online RL-VLA training

### Test-time RL-VLA

Test-time RL-VLA adapts behavior during deployment through lightweight updates, addressing the expensive cost of full model fine-tuning in real-world scenarios.

**Key Adaptation Mechanisms:**
- **Value Guidance**: Using pre-trained value functions to influence action selection
- **Memory Buffer Guidance**: Retrieving relevant historical experiences during inference
- **Planning-guided Adaptation**: Explicit reasoning over future action sequences

## ğŸ“š Paper Collection

### Legend
- **Action**: AR (Autoregressive), Diffusion, Flow (Flow-matching)
- **Reward**: D (Dense Reward), S (Sparse Reward)
- **Type**: MB (Model-based), MF (Model-free)
- **Environment**: Sim. (Simulation), Real (Real-world)

### Offline RL-VLA

| Method | Date | Sim. | Real | Base VLA Model | Action | Reward | Algorithm | Type | Project |
|--------|------|------|------|----------------|--------|---------|-----------|------|---------|
| [Q-Transformer](https://arxiv.org/abs/2309.10150) | 2023.10 | âœ“ | âœ— | Transformer | AR | S | CQL | MF | [ğŸ”—](https://qtransformer.github.io/) |
| [PAC](https://arxiv.org/abs/2402.05546) | 2024.02 | âœ“ | âœ“ | Perceiver-Actor-Critic | AR | S | AC | MF | [ğŸ”—](https://sites.google.com/view/perceiver-actor-critic) |
| [ConRFT](https://arxiv.org/pdf/2502.05450) | 2025.04 | âœ— | âœ“ | Octo-small | Diffusion | S | Cal-QL + BC | MF | [ğŸ”—](https://github.com/cccedric/conrft) |
| [ReinboT](https://icml.cc/virtual/2025/poster/45523) | 2025.05 | âœ“ | âœ“ | ReinboT | AR | D | DT + RTG | MF | - |
| [CO-RFT](https://arxiv.org/pdf/2508.02219) | 2025.08 | âœ— | âœ“ | RoboVLMs | AR | D | Cal-QL + TD3 | MF | - |
| [ARFM](https://arxiv.org/pdf/2509.04063) | 2025.09 | âœ“ | âœ“ | Ï€â‚€ | Flow | D | ARFM | MF | - |

### Online RL-VLA

| Method | Date | Sim. | Real | Base VLA Model | Action | Reward | Algorithm | Type | Project |
|--------|------|------|------|----------------|--------|---------|-----------|------|---------|
| [FLaRe](https://arxiv.org/abs/2409.16578) | 2024.09 | âœ“ | âœ“ | SPOC | AR | S | PPO | MF | [ğŸ”—](https://github.com/JiahengHu/FLaRe) |
| [PA-RL](https://arxiv.org/abs/2412.06685) | 2024.12 | âœ“ | âœ“ | OpenVLA | AR | S | PA-RL | MF | [ğŸ”—](https://policyagnosticrl.github.io/) |
| [RLDG](https://arxiv.org/pdf/2412.09858) | 2024.12 | âœ— | âœ“ | OpenVLA / Octo | AR / Diffusion | S | RLPD | MF | [ğŸ”—](https://generalist-distillation.github.io/) |
| [iRe-VLA](https://arxiv.org/abs/2501.16664) | 2025.01 | âœ“ | âœ“ | iRe-VLA | AR | S | SACfD + SFT | MF | - |
| [GRAPE](https://arxiv.org/pdf/2411.19309) | 2025.02 | âœ“ | âœ“ | OpenVLA | AR | D | TPO | MF | [ğŸ”—](https://github.com/aiming-lab/grape) |
| [SafeVLA](https://arxiv.org/abs/2503.03480) | 2025.03 | âœ“ | âœ— | SPOC | AR | S | PPO | MF | [ğŸ”—](https://sites.google.com/view/pku-safevla) |
| [ConRFT](https://arxiv.org/pdf/2502.05450) | 2025.04 | âœ— | âœ“ | Octo-small | Diffusion | S | Cal-QL + BC | MF | [ğŸ”—](https://github.com/cccedric/conrft) |
| [RIPT-VLA](https://arxiv.org/abs/2505.17016) | 2025.05 | âœ“ | âœ— | QueST / OpenVLA-OFT | AR | S | LOOP | MF | [ğŸ”—](https://ariostgx.github.io/ript_vla/) |
| [VLA-RL](https://arxiv.org/abs/2505.18719) | 2025.05 | âœ“ | âœ— | OpenVLA | AR | D | PPO | MF | [ğŸ”—](https://github.com/GuanxingLu/vlarl) |
| [RLVLA](https://arxiv.org/abs/2505.19789) | 2025.05 | âœ“ | âœ— | OpenVLA | AR | S | PPO / GRPO / DPO | MF | [ğŸ”—](https://github.com/gen-robot/RL4VLA) |
| [RFTF](https://arxiv.org/abs/2505.19767) | 2025.05 | âœ“ | âœ— | GR-MG, Seer | AR | D | PPO | MF | - |
| [TGRPO](https://arxiv.org/abs/2506.08440) | 2025.06 | âœ“ | âœ— | OpenVLA | AR | D | GRPO | MF | - |
| [RLRC](https://arxiv.org/pdf/2506.17639) | 2025.06 | âœ“ | âœ— | OpenVLA | AR | S | PPO | MF | [ğŸ”—](https://rlrc-vla.github.io/) |
| [SimpleVLA-RL](https://arxiv.org/pdf/2509.09674) | 2025.09 | âœ“ | âœ“ | OpenVLA-OFT | AR | S | GRPO | MF | [ğŸ”—](https://github.com/PRIME-RL/SimpleVLA-RL) |
| [Dual-Actor FT](https://arxiv.org/pdf/2509.13774) | 2025.09 | âœ“ | âœ“ | Octo / SmolVLA | Diffusion | S | QL + BC | MF | [ğŸ”—](https://sites.google.com/view/hil-daft/) |
| [Generalist](https://arxiv.org/pdf/2509.15155) | 2025.09 | âœ“ | âœ“ | PaLI 3B | AR | D | REINFORCE | MF | [ğŸ”—](https://self-improving-efms.github.io./) |
| [VLAC](https://arxiv.org/abs/2509.15937) | 2025.09 | âœ— | âœ“ | VLAC | AR | D | PPO | MF | [ğŸ”—](https://github.com/InternRobotics/VLAC) |
| [AC PPO](https://arxiv.org/pdf/2509.25718) | 2025.09 | âœ“ | âœ— | Octo-small | AR | S | PPO+BC | MF | - |
| [VLA-RFT](https://arxiv.org/abs/2510.00406) | 2025.10 | âœ“ | âœ— | VLA-Adapter | Flow | D | GRPO | MB | [ğŸ”—](https://vla-rft.github.io/) |
| [RLinf-VLA](https://arxiv.org/pdf/2510.06710v1) | 2025.10 | âœ“ | âœ“ | OpenVLA / OpenVLA-OFT | AR | S | PPO / GRPO | MF | [ğŸ”—](https://github.com/RLinf/RLinf) |
| [FPO](https://arxiv.org/pdf/2510.09976) | 2025.10 | âœ“ | âœ— | Ï€â‚€ | Flow | S | FPO | MF | - |
| [ReSA](https://arxiv.org/pdf/2510.12710) | 2025.10 | âœ“ | âœ— | OpenVLA | AR | D | PPO + SFT | MF | - |
| [Ï€_RL](https://arxiv.org/abs/2510.25889) | 2025.10 | âœ“ | âœ— | Ï€â‚€ / Ï€â‚€.â‚… | Flow | S | PPO / GRPO | MF | [ğŸ”—](https://github.com/RLinf/RLinf) |
| [PLD](https://arxiv.org/abs/2511.00091) | 2025.10 | âœ“ | âœ“ | OpenVLA / Ï€â‚€ / Octo | AR / Flow | S | Cal-QL + SAC | MF | [ğŸ”—](https://www.wenlixiao.com/self-improve-VLA-PLD) |
| [World-Env](https://arxiv.org/abs/2509.24948) | 2025.11 | âœ“ | âœ“ | OpenVLA-OFT | AR | D | PPO | MB | [ğŸ”—](https://github.com/amap-cvlab/world-env) |
| [RobustVLA](https://arxiv.org/pdf/2511.01331) | 2025.11 | âœ“ | âœ— | OpenVLA-OFT | AR | D | PPO | MF | - |
| [WMPO](https://arxiv.org/abs/2511.09515) | 2025.11 | âœ“ | âœ“ | OpenVLA-OFT | AR | S | GRPO | MB | [ğŸ”—](https://wm-po.github.io/) |

### Test-time RL-VLA

| Method | Date | Sim. | Real | Base VLA Model | Action | Reward | Algorithm | Type | Project |
|--------|------|------|------|----------------|--------|---------|-----------|------|---------|
| [V-GPS](https://arxiv.org/abs/2410.13816) | 2024.10 | âœ“ | âœ“ | Octo / RT-1 / OpenVLA | AR | D | Cal-QL | MF | [ğŸ”—](https://github.com/nakamotoo/V-GPS) |
| [Hume](https://arxiv.org/abs/2505.21432) | 2025.06 | âœ“ | âœ“ | Hume | Flow | S | Value Guidance | MF | [ğŸ”—](https://github.com/hume-vla/hume) |
| [VLA-Reasoner](https://arxiv.org/abs/2509.22643) | 2025.09 | âœ“ | âœ“ | OpenVLA / SpatialVLA et al. | AR / Diffusion | D | MCTS | MB | - |

## ğŸ”— Useful Resources

### Base VLA Models
- [OpenVLA](https://github.com/openvla/openvla) - Open-source VLA model
- [Octo](https://github.com/octo-models/octo) - Generalist robot policy
- [RT-1](https://github.com/google-research/robotics_transformer) - Robotics Transformer

### Datasets & Benchmarks
- [Open X-Embodiment](https://robotics-transformer-x.github.io/) - Large-scale robotic datasets
- [LIBERO](https://libero-ai.github.io/) - Benchmark for lifelong robot learning

### Frameworks & Tools
- [RLinf](https://github.com/RLinf/RLinf) - Infrastructure for online RL fine-tuning of VLAs

## ğŸ“Š Statistics

- **Total Papers**: 45+
- **Offline RL-VLA**: 6 papers
- **Online RL-VLA**: 26 papers  
- **Test-time RL-VLA**: 3 papers
- **Papers with Project Pages**: 20+
- **Last Updated**: November 2025

**Note**: The ğŸ”— symbol in the Project column indicates papers with available project pages, GitHub repositories, or demo websites.

## ğŸ¤ Contributing

We welcome contributions to this awesome list! Please feel free to:

1. **Add new papers**: Submit a PR with new RL-VLA papers following the existing format
2. **Update information**: Correct any errors or update paper information
3. **Suggest improvements**: Propose better organization or additional sections

### Contribution Guidelines
- Ensure papers are relevant to RL-VLA research
- Include paper links, project pages (if available), and key details
- Follow the existing table format for consistency
- Add a brief description for new paradigms or significant methodological contributions

## ğŸ“„ Citation

If you find this repository useful, please consider citing:

```bibtex
@article{deng2025rlvla,
  title={A Survey on Reinforcement Learning of Vision-Language-Action Models for Robotic Manipulation},
  author={Haoyuan Deng, Zhenyu Wu, Haichao Liu, Wenkai Guo, Yuquan Xue, Ziyu Shan, Chuanrui Zhang, Bofang Jia, Yuan Ling, Guanxing Lu, and Ziwei Wangâ€ },
  journal={arXiv preprint arXiv:},
  year={2025}
}
```


---

â­ **Star this repository** if you find it helpful!

ğŸ”„ **Watch** for updates on the latest RL-VLA research!